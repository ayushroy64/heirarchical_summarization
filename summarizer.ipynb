{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85cc1f8-7ca7-4f0f-bdf8-441e6f5b5017",
   "metadata": {},
   "source": [
    "### Extract Code Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe768ee-5268-4d9c-8c49-c02b2c4bc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def extract_code_structure(code: str, max_chars: int = 2000) -> Tuple[List[str], Dict[str, List[str]], Dict[str, List[str]]]:\n",
    "    code_chunks = []\n",
    "    call_graph = defaultdict(list)\n",
    "    class_hierarchy = defaultdict(list)\n",
    "\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        for node in ast.walk(tree):\n",
    "\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                func_code = ast.get_source_segment(code, node)\n",
    "                if func_code and len(func_code) <= max_chars:\n",
    "                    code_chunks.append((node.name, func_code))\n",
    "                for sub in ast.walk(node):\n",
    "                    if isinstance(sub, ast.Call) and isinstance(sub.func, ast.Name):\n",
    "                        call_graph[node.name].append(sub.func.id)\n",
    "\n",
    "            elif isinstance(node, ast.ClassDef):\n",
    "                for item in node.body:\n",
    "                    if isinstance(item, ast.FunctionDef):\n",
    "                        class_hierarchy[node.name].append(item.name)\n",
    "                        method_code = ast.get_source_segment(code, item)\n",
    "                        if method_code and len(method_code) <= max_chars:\n",
    "                            code_chunks.append((item.name, method_code))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return code_chunks, dict(call_graph), dict(class_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d93824e6-5e6c-447d-ace4-2da622c95ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def extract_functions_from_code(code: str):\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        return {\n",
    "            node.name: ast.get_source_segment(code, node)\n",
    "            for node in ast.walk(tree)\n",
    "            if isinstance(node, ast.FunctionDef)\n",
    "        }\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "def extract_call_graph(code: str):\n",
    "    call_graph = {}\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                callers = []\n",
    "                for child in ast.walk(node):\n",
    "                    if isinstance(child, ast.Call) and hasattr(child.func, \"id\"):\n",
    "                        callers.append(child.func.id)\n",
    "                call_graph[node.name] = list(set(callers))\n",
    "    except:\n",
    "        pass\n",
    "    return call_graph\n",
    "\n",
    "def extract_class_hierarchy(code: str):\n",
    "    class_map = {}\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.ClassDef):\n",
    "                methods = [n.name for n in node.body if isinstance(n, ast.FunctionDef)]\n",
    "                class_map[node.name] = methods\n",
    "    except:\n",
    "        pass\n",
    "    return class_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aaa5ca-dcf5-4fa1-a248-d7f93c82ae28",
   "metadata": {},
   "source": [
    "### Summarization Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff80a37-22ad-4592-ae85-2354502c12e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Setup device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# ========== FUNCTION-LEVEL MODEL: CodeT5 ========== #\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base-multi-sum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-base-multi-sum\")\n",
    "\n",
    "func_summarizer = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "# ========== EMBEDDING MODEL: SBERT ========== #\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ========== FILE/REPO-LEVEL MODEL: Longformer (LED) ========== #\n",
    "file_summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"allenai/led-base-16384\",\n",
    "    tokenizer=\"allenai/led-base-16384\",\n",
    "    device=device,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    min_length=64\n",
    ")\n",
    "\n",
    "# ========== GRAPH-AWARE PROMPT BUILDER ========== #\n",
    "def format_graph_context(func_summaries, call_graph=None, class_hierarchy=None):\n",
    "    \"\"\"\n",
    "    Format function summaries and graph information into a readable structured input.\n",
    "    \"\"\"\n",
    "    lines = [\"You are summarizing a Python module.\"]\n",
    "\n",
    "    if func_summaries:\n",
    "        lines.append(\"Function Summaries:\")\n",
    "        for name, summary in func_summaries.items():\n",
    "            lines.append(f\"- {name}: {summary}\")\n",
    "\n",
    "    if call_graph:\n",
    "        lines.append(\"\\nCall Graph:\")\n",
    "        for caller, callees in call_graph.items():\n",
    "            if callees:\n",
    "                lines.append(f\"- {caller} â†’ {', '.join(callees)}\")\n",
    "\n",
    "    if class_hierarchy:\n",
    "        lines.append(\"\\nClass Hierarchy:\")\n",
    "        for cls, methods in class_hierarchy.items():\n",
    "            lines.append(f\"- {cls}: [{', '.join(methods)}]\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb7c66-8d5a-4d30-b963-643b5bbaf992",
   "metadata": {},
   "source": [
    "### Graph Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a5792d-75ec-4259-807b-88671d61ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import util\n",
    "\n",
    "def rank_by_graph_and_embedding(summaries: Dict[str, str], call_graph: Dict[str, List[str]], top_k: int = 5):\n",
    "    # Graph degree as rough importance\n",
    "    graph_score = {name: len(callees) for name, callees in call_graph.items()}\n",
    "    \n",
    "    # Embedding richness\n",
    "    texts = list(summaries.values())\n",
    "    names = list(summaries.keys())\n",
    "    embeddings = embed_model.encode(texts, convert_to_tensor=True)\n",
    "    norms = embeddings.norm(dim=1).cpu().tolist()\n",
    "\n",
    "    # Combine scores\n",
    "    combined = []\n",
    "    for i, name in enumerate(names):\n",
    "        score = 0.5 * graph_score.get(name, 0) + 0.5 * norms[i]\n",
    "        combined.append((name, summaries[name], score))\n",
    "\n",
    "    # Select top-k\n",
    "    combined.sort(key=lambda x: x[2], reverse=True)\n",
    "    return [summary for _, summary, _ in combined[:top_k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ea566-f008-438e-80b1-34da8ef5b7cb",
   "metadata": {},
   "source": [
    "### Summarize File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861fbfef-68c2-4e9c-b3ed-6be6d48cd4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize_file_with_graph(code: str, top_k: int = 5):\n",
    "#     chunks, call_graph, _ = extract_code_structure(code)\n",
    "#     if not chunks:\n",
    "#         return \"No valid chunks\"\n",
    "\n",
    "#     names, funcs = zip(*chunks)\n",
    "#     results = func_summarizer(list(funcs), max_length=64, truncation=True, do_sample=False)\n",
    "#     summaries = {name: res[\"generated_text\"].strip() for name, res in zip(names, results)}\n",
    "\n",
    "#     top_summaries = rank_by_graph_and_embedding(summaries, call_graph, top_k)\n",
    "#     return file_summarizer(\" \".join(top_summaries), max_length=128, min_length=64, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "def summarize_file_with_graph(code_text: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Summarize a file by:\n",
    "    - Extracting function-level summaries using CodeT5\n",
    "    - Building call/class hierarchy (graph_utils)\n",
    "    - Creating structured prompt for LED summarizer\n",
    "    \"\"\"\n",
    "    functions = extract_functions_from_code(code_text)\n",
    "    if not functions:\n",
    "        return \"No functions found.\"\n",
    "\n",
    "    func_names = list(functions.keys())\n",
    "    func_bodies = list(functions.values())\n",
    "\n",
    "    # Summarize functions\n",
    "    func_summaries_raw = func_summarizer(func_bodies, max_length=64, do_sample=False)\n",
    "    func_summaries = {\n",
    "        func_names[i]: func_summaries_raw[i][\"generated_text\"].strip()\n",
    "        for i in range(len(func_names))\n",
    "    }\n",
    "\n",
    "    # Select top-k longest functions as proxy for importance\n",
    "    top_funcs = sorted(func_summaries.items(), key=lambda x: len(functions[x[0]]), reverse=True)[:top_k]\n",
    "    top_func_summaries = {k: v for k, v in top_funcs}\n",
    "\n",
    "    # Build graph context\n",
    "    call_graph = extract_call_graph(code_text)\n",
    "    class_hierarchy = extract_class_hierarchy(code_text)\n",
    "    input_text = format_graph_context(top_func_summaries, call_graph, class_hierarchy)\n",
    "\n",
    "    # Summarize with LED\n",
    "    summary = file_summarizer(\n",
    "        input_text,\n",
    "        max_length=128,\n",
    "        min_length=64,\n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=False,\n",
    "    )[0][\"summary_text\"]\n",
    "\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8612796-d65a-460a-8d11-2813367061ee",
   "metadata": {},
   "source": [
    "### Summarize Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e95a0cb2-04ec-43dd-8168-978997246b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize_repo_with_graph(file_contents: Dict[str, str], top_files: int = 5, top_k_funcs: int = 5):\n",
    "#     file_summaries = []\n",
    "#     for file_path, code in list(file_contents.items())[:top_files]:\n",
    "#         try:\n",
    "#             summary = summarize_file_with_graph(code, top_k=top_k_funcs)\n",
    "#             file_summaries.append(summary)\n",
    "#         except Exception:\n",
    "#             continue\n",
    "\n",
    "#     if not file_summaries:\n",
    "#         return \"No summary generated\"\n",
    "\n",
    "#     repo_input = \" \".join(\" \".join(s.split()[:1024]) for s in file_summaries)\n",
    "#     return file_summarizer(repo_input, max_length=256, min_length=100, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "\n",
    "def summarize_repo_with_graph(file_dict: dict, top_files=5, top_k_funcs=5):\n",
    "    \"\"\"\n",
    "    Summarize a repository:\n",
    "    - Summarize each file using summarize_file_with_graph\n",
    "    - Combine top-k summaries\n",
    "    - Feed to LED summarizer\n",
    "    \"\"\"\n",
    "    file_summaries = []\n",
    "\n",
    "    for file_path, code_text in list(file_dict.items())[:top_files]:\n",
    "        try:\n",
    "            summary = summarize_file_with_graph(code_text, top_k=top_k_funcs)\n",
    "            file_summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped file {file_path} due to: {e}\")\n",
    "\n",
    "    if not file_summaries:\n",
    "        return \"No valid summaries found.\"\n",
    "\n",
    "    combined_input = \"\\n\\n\".join(file_summaries)\n",
    "    final_summary = file_summarizer(\n",
    "        combined_input,\n",
    "        max_length=256,\n",
    "        min_length=100,\n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=False,\n",
    "    )[0][\"summary_text\"]\n",
    "\n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a4f93f-7429-4902-9ad7-62152a7ccfa6",
   "metadata": {},
   "source": [
    "### Evaluate Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9804f803-a20c-433c-bbf3-f0769e26d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from sentence_transformers import util\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load HF metrics once\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def cosine_similarity(preds, refs, batch_size=2):\n",
    "    pred_embeds = embed_model.encode(preds, convert_to_tensor=True, batch_size=batch_size)\n",
    "    ref_embeds = embed_model.encode(refs, convert_to_tensor=True, batch_size=batch_size)\n",
    "    return util.cos_sim(pred_embeds, ref_embeds).diag().mean().item()\n",
    "\n",
    "# ---------------- FUNCTION LEVEL ---------------- #\n",
    "def evaluate_function_level(preds: list, refs: list):\n",
    "    rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
    "    bert = bertscore.compute(predictions=preds, references=refs, lang=\"en\")\n",
    "    bert_f1 = sum(bert[\"f1\"]) / len(bert[\"f1\"])\n",
    "\n",
    "    print(\"\\nFunction-Level ROUGE:\")\n",
    "    for metric in [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]:\n",
    "        print(f\"{metric}: {rouge_scores[metric]:.4f}\")\n",
    "    \n",
    "    print(f\"Function-Level BERTScore: {bert_f1:.4f}\")\n",
    "    return rouge_scores, bert_f1\n",
    "\n",
    "# ---------------- FILE/REPO LEVEL ---------------- #\n",
    "def evaluate_unsupervised_level(pred: str, ref: str, label: str = \"Repo\"):\n",
    "    bert = bertscore.compute(predictions=[pred], references=[ref], lang=\"en\")[\"f1\"][0]\n",
    "    cos = cosine_similarity([pred], [ref])\n",
    "\n",
    "    print(f\"\\n{label}-Level Evaluation:\")\n",
    "    print(f\"BERTScore: {bert:.4f}\")\n",
    "    print(f\"Cosine Similarity: {cos:.4f}\")\n",
    "\n",
    "    return bert, cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6dc095-9218-4b14-8767-1463e0850755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"code_search_net\", \"python\", split=\"train[:1%]\")\n",
    "# dataset = dataset.filter(lambda x: x[\"func_code_string\"])\n",
    "\n",
    "# # Build repo â†’ file â†’ code\n",
    "# from collections import defaultdict\n",
    "# repo_map = defaultdict(lambda: defaultdict(str))\n",
    "# for item in dataset:\n",
    "#     repo = item[\"repository_name\"]\n",
    "#     path = item[\"func_path_in_repository\"]\n",
    "#     repo_map[repo][path] += \"\\n\" + item[\"func_code_string\"]\n",
    "\n",
    "# # Pick top repo\n",
    "# repo_name = list(repo_map.keys())[0]\n",
    "# files = repo_map[repo_name]\n",
    "# print(f\"Summarizing {repo_name}...\")\n",
    "\n",
    "# # Run summarization\n",
    "# summary = summarize_repo_with_graph(files, top_files=5, top_k_funcs=5)\n",
    "# raw_code = \"\\n\".join(files.values())[:5000]\n",
    "\n",
    "# # Evaluate\n",
    "# bert, cos = evaluate_summary(summary, raw_code)\n",
    "# print(\"\\nREPO SUMMARY:\\n\", summary)\n",
    "# print(f\"\\nBERTScore: {bert:.4f} | Cosine Similarity: {cos:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aac612-b8cb-4ec9-ab36-f8f060191d3b",
   "metadata": {},
   "source": [
    "### Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed679855-52ba-47bc-9c49-a47e91916f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ FUNCTION LEVEL EVALUATION ================\n",
      "\n",
      "\n",
      "ðŸ” Function-Level ROUGE:\n",
      "rouge1: 0.4376\n",
      "rouge2: 0.3668\n",
      "rougeL: 0.4244\n",
      "rougeLsum: 0.4346\n",
      "ðŸ” Function-Level BERTScore: 0.8576\n",
      "\n",
      "================ FILE LEVEL EVALUATION ================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 109. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” File-Level Evaluation:\n",
      "BERTScore: 0.7810\n",
      "Cosine Similarity: 0.6971\n",
      "\n",
      "ðŸ” File-Level Evaluation:\n",
      "BERTScore: 0.7890\n",
      "Cosine Similarity: 0.3769\n",
      "\n",
      "ðŸ” File-Level Evaluation:\n",
      "BERTScore: 0.7721\n",
      "Cosine Similarity: 0.4991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” File-Level Evaluation:\n",
      "BERTScore: 0.7643\n",
      "Cosine Similarity: 0.5989\n",
      "\n",
      "ðŸ” File-Level Evaluation:\n",
      "BERTScore: 0.7748\n",
      "Cosine Similarity: 0.6304\n",
      "\n",
      "ðŸ” File-Level Evaluation:\n",
      "BERTScore: 0.7658\n",
      "Cosine Similarity: 0.5190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 105. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” File-Level Evaluation:\n",
      "BERTScore: 0.7733\n",
      "Cosine Similarity: 0.5520\n",
      "\n",
      "ðŸ” File-Level Evaluation:\n",
      "BERTScore: 0.7504\n",
      "Cosine Similarity: 0.5895\n",
      "\n",
      "âœ… Avg File BERTScore: 0.7714\n",
      "âœ… Avg File Cosine Similarity: 0.5579\n",
      "\n",
      "================ REPO LEVEL EVALUATION ================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 109. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\n",
      "Your max_length is set to 256, but your input_length is only 253. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=126)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Repo-Level Evaluation:\n",
      "BERTScore: 0.7812\n",
      "Cosine Similarity: 0.7010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 128, but your input_length is only 62. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Repo-Level Evaluation:\n",
      "BERTScore: 0.7665\n",
      "Cosine Similarity: 0.5092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 105. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n",
      "Your max_length is set to 128, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Repo-Level Evaluation:\n",
      "BERTScore: 0.7678\n",
      "Cosine Similarity: 0.4784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 128, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Repo-Level Evaluation:\n",
      "BERTScore: 0.7601\n",
      "Cosine Similarity: 0.4561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Your max_length is set to 128, but your input_length is only 111. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n",
      "Your max_length is set to 128, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Repo-Level Evaluation:\n",
      "BERTScore: 0.7798\n",
      "Cosine Similarity: 0.4907\n",
      "\n",
      "âœ… Avg Repo BERTScore: 0.7711\n",
      "âœ… Avg Repo Cosine Similarity: 0.5271\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load and group data\n",
    "dataset = load_dataset(\"code_search_net\", \"python\", split=\"train[:1%]\")\n",
    "dataset = dataset.filter(lambda x: x[\"func_code_string\"] and x[\"func_documentation_string\"])\n",
    "\n",
    "# Group repo â†’ file â†’ code\n",
    "repo_map = defaultdict(lambda: defaultdict(list))\n",
    "for item in dataset:\n",
    "    repo_map[item[\"repository_name\"]][item[\"func_path_in_repository\"]].append(item[\"func_code_string\"])\n",
    "\n",
    "# ========== FUNCTION-LEVEL (First 100 functions) ==========\n",
    "\n",
    "print(\"\\n================ FUNCTION LEVEL EVALUATION ================\\n\")\n",
    "\n",
    "func_codes = [item[\"func_code_string\"] for item in dataset.select(range(100))]\n",
    "func_refs = [item[\"func_documentation_string\"] for item in dataset.select(range(100))]\n",
    "func_preds = [out[\"generated_text\"].strip() for out in func_summarizer(func_codes, max_length=64, do_sample=False)]\n",
    "\n",
    "evaluate_function_level(func_preds, func_refs)\n",
    "\n",
    "# ========== FILE-LEVEL (Top 3 repos Ã— 3 files each) ==========\n",
    "\n",
    "print(\"\\n================ FILE LEVEL EVALUATION ================\\n\")\n",
    "\n",
    "file_bert_scores = []\n",
    "file_cos_scores = []\n",
    "\n",
    "for repo_name in list(repo_map.keys())[:3]:\n",
    "    for file_path, func_list in list(repo_map[repo_name].items())[:3]:\n",
    "        raw_code = \"\\n\".join(func_list)\n",
    "        try:\n",
    "            file_summary = summarize_file_with_graph(raw_code, top_k=5)\n",
    "            bert, cos = evaluate_unsupervised_level(file_summary, raw_code, label=\"File\")\n",
    "            file_bert_scores.append(bert)\n",
    "            file_cos_scores.append(cos)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped file {file_path} due to: {e}\")\n",
    "\n",
    "print(f\"\\nAvg File BERTScore: {sum(file_bert_scores)/len(file_bert_scores):.4f}\")\n",
    "print(f\"Avg File Cosine Similarity: {sum(file_cos_scores)/len(file_cos_scores):.4f}\")\n",
    "\n",
    "# ========== REPO-LEVEL (Top 5 repos) ==========\n",
    "\n",
    "print(\"\\n================ REPO LEVEL EVALUATION ================\\n\")\n",
    "\n",
    "repo_bert_scores = []\n",
    "repo_cos_scores = []\n",
    "\n",
    "for repo_name in list(repo_map.keys())[:5]:\n",
    "    file_contents = {\n",
    "        path: \"\\n\".join(funcs)\n",
    "        for path, funcs in list(repo_map[repo_name].items())[:5]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        repo_summary = summarize_repo_with_graph(file_contents, top_files=5, top_k_funcs=5)\n",
    "        raw_repo_code = \"\\n\".join(file_contents.values())[:5000]\n",
    "        bert, cos = evaluate_unsupervised_level(repo_summary, raw_repo_code, label=\"Repo\")\n",
    "        repo_bert_scores.append(bert)\n",
    "        repo_cos_scores.append(cos)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped repo {repo_name} due to: {e}\")\n",
    "\n",
    "print(f\"\\nAvg Repo BERTScore: {sum(repo_bert_scores)/len(repo_bert_scores):.4f}\")\n",
    "print(f\"Avg Repo Cosine Similarity: {sum(repo_cos_scores)/len(repo_cos_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a4b20-1445-48ba-9308-237a4ac113f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
